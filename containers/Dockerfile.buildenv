# Distributed under the MIT License.
# See LICENSE.txt for details.

# If you change this file please push the new images to DockerHub. If you do not
# have permission to push to DockerHub please coordinate with someone who does.
# Since changes to this image affect our testing infrastructure it is important
# all changes be carefully reviewed before they are pushed.
#
# GitHub Actions (recommended method)
#
# This is how you use GitHub Actions to build and push the new images:
#
# 1. Push your changes to the `develop` branch on your fork of the spectre
#    repository on GitHub (https://github.com/YOURNAME/spectre).
# 2. Go to https://github.com/YOURNAME/spectre/actions/workflows/BuildDockerContainer.yaml.
# 3. Select "Run workflow". Here you can select the location on DockerHub where
#    you want the new images pushed. Select `sxscollaboration/spectre` (which is
#    used by CI) or a location on your own DockerHub account. Either way, you
#    need to set the `DOCKERHUB_USERNAME` and `DOCKERHUB_TOKEN` secrets in the
#    repository to credentials that have write access.
# 4. The images will be built and pushed to DockerHub and CI will run over them.
#    See `.github/workflows/BuildDockerContainer.yaml` for details.
#
# Local build & push (can be tedious and time consuming)
#
# What you need to push to Dockerhub is:
#
#   Arch: linux/amd64,linux/arm64
#         # Note that cross compiling can be slow, so for testing just do amd64
#         # or arm64 depending on your CPU. Docker builds the image in a way
#         # that when someone clones it, depending on their CPU architecture,
#         # they will get either the amd64 or arm64 version.
#   Ubuntu version: 22.04 (default)
#   --platform linux/amd64,linux/arm64
#   --target dev
#   -t sxscollaboration/spectre:dev
#
#   Arch: linux/amd64
#   Ubuntu version: 18.04
#   --platform linux/amd64
#   --target dev
#   -t sxscollaboration/spectre:dev1804
#   --build-arg UBUNTU_VERSION=18.04
#
# Docker must be run as root on your machine. There are 4 different images in
# this Dockerfile, but you can ignore the 'deploy_static_execs_and_libs' since
# it's only used in CI. Here is how to build them for x86_64 (for Apple Silicon,
# replace linux/amd64 with linux/arm64):
#
# 1. `dev`
#
#   cd $SPECTRE_HOME
#   docker build --target dev -t sxscollaboration/spectre:dev \
#                 --platform linux/amd64 \
#                 -f ./containers/Dockerfile.buildenv .
#
#   # Cross compiling for AMD64 and ARM64:
#   #
#   # To check if you can cross compile, first run
#   #   docker buildx ls
#   # This will look something like:
#   #  NAME/NODE        DRIVER/ENDPOINT                   STATUS    BUILDKIT   \
#   #    PLATFORMS
#   # default*          docker
#   #  \_ default       \_ default                       running   v0.13.2    \
#   #     linux/amd64, linux/amd64/v2, linux/amd64/v3, linux/386
#   #
#   # Note that the DRIVER/ENDPOINT is 'docker'. We want to instead use
#   # 'docker-container'. To do this, run
#   #    docker buildx create --name mybuilder --bootstrap --use
#   # When you run 'docker buildx ls' now you should see
#   #  mybuilder*       docker-container
#   # The asterisk means docker will use this builder. You are now ready to
#   # cross-compile code.
#   #
#   # This is documented at:
#   #   https://docs.docker.com/build/building/multi-platform/#getting-started
#
#   docker buildx build --target dev -t sxscollaboration/spectre:dev \
#                 --platform linux/amd64,linux/arm64 \
#                 -f ./containers/Dockerfile.buildenv --push .
#
# You DO NOT need to push the below. CI does that automatically.
#
# 3. `demo` To build this, you MUST first push the `dev` images to
#           DockerHub as the `demo` image uses the remote `dev` image.
#
#   docker push sxscollaboration/spectre:dev
#
#    To build `demo`, you must be in $SPECTRE_ROOT and there cannot be a
#    directory named `build` in $SPECTRE_ROOT because the image will create
#    this directory (in the container).
#
#   cd $SPECTRE_HOME
#   rm -rf build/
#   docker build --target demo -t sxscollaboration/spectre:demo \
#                 --platform linux/amd64 \
#                 -f ./containers/Dockerfile.buildenv .
#
#    and then to push the `demo` image to DockerHub:
#
#   docker push sxscollaboration/spectre:demo
#
# 4. `deploy_static_execs_and_libs` This is used for building various
#    executables and libraries with an old enough version of glibc so
#    that the resulting executables are quite portable. This must be built
#    as, but should generally only be done in CI unless you are debugging
#    the container build.
#
#  docker build --target deploy_static_execs_and_libs --platform linux/amd64 \
#    --build-arg UBUNTU_VERSION=18.04 -f ./containers/Dockerfile.buildenv .
#
#    That is, it uses Ubuntu 18.04 and AMD64. There is no support for ARM and
#    newer versions of Ubuntu will limit compatibility.
ARG UBUNTU_VERSION=22.04
FROM ubuntu:${UBUNTU_VERSION} AS base
ARG UBUNTU_VERSION=22.04

# See
# https://docs.docker.com/engine/reference/builder/#automatic-platform-args-in-the-global-scope
# for how TARGETARCH is defined.
ARG TARGETARCH

# Install add-apt-repository and basic tools
RUN if [ ${UBUNTU_VERSION} = 18.04 ] && [ "$TARGETARCH" = "arm64" ]; then \
    echo "Cannot use Ubuntu 18.04 with ARM" && exit 1; fi && apt-get update -y \
    && apt-get install -y software-properties-common curl wget git file \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    add-apt-repository ppa:ubuntu-toolchain-r/test; fi

# Add LLVM apt repository for newer versions of clang
RUN wget -qO- https://apt.llvm.org/llvm-snapshot.gpg.key | \
    tee /etc/apt/trusted.gpg.d/apt.llvm.org.asc \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    add-apt-repository -y \
    'deb http://apt.llvm.org/bionic/ llvm-toolchain-bionic-17 main'; \
    elif [ ${UBUNTU_VERSION} = 22.04 ]; then \
    add-apt-repository -y \
    'deb http://apt.llvm.org/jammy/ llvm-toolchain-jammy-16 main'; \
    else \
    exit 1; \
    fi

# Install compilers and build tools
RUN apt-get update -y \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    apt-get install -y gcc-9 g++-9 gfortran-9 clang-17; \
    else \
    apt-get install -y gcc g++ gfortran clang clang-format clang-tidy cmake; \
    fi \
    && apt-get install -y lld bison flex libncurses-dev gdb autoconf automake \
    ninja-build lcov \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    wget -O cmake-install.sh \
    "https://github.com/Kitware/CMake/releases/download/v3.28.1/cmake-3.28.1-linux-x86_64.sh" \
    && chmod +x ./cmake-install.sh && ./cmake-install.sh --skip-license \
    --prefix=/usr/local && rm cmake-install.sh; \
    fi

# Install SpECTRE dependencies that are available through apt
#
# We intentionally don't install libboost-all-dev because that installs
# Boost.MPI, which installs OpenMPI into the container. When MPI is
# installed inside the container it makes it very difficult to use
# Singularity on HPC systems to interface with the system MPI library.
# The system MPI libraries are usually configured to take advantage of
# InfiniBand or various other networking layers.
RUN apt-get update -y \
    && apt-get install -y \
        libopenblas-dev liblapack-dev \
        libgsl-dev \
        libboost-dev libboost-program-options-dev \
        libboost-thread-dev libboost-tools-dev libssl-dev \
        libhdf5-dev hdf5-tools \
        libarpack2-dev \
        libfftw3-dev \
        libbenchmark-dev \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    wget https://github.com/jbeder/yaml-cpp/archive/refs/tags/0.8.0.tar.gz \
    && tar xf 0.8.0.tar.gz && mkdir ./yaml-cpp-0.8.0/build \
    && cd ./yaml-cpp-0.8.0/build \
    && cmake \
    -D CMAKE_C_COMPILER=gcc-9 \
    -D CMAKE_CXX_COMPILER=g++-9 \
    -D CMAKE_BUILD_TYPE=Release \
    -D YAML_BUILD_SHARED_LIBS=OFF \
    -D BUILD_SHARED_LIBS=OFF \
    -D YAML_ENABLE_PIC=ON \
    -D POSITION_INDEPENDENT_CODE=ON \
    -D YAML_CPP_BUILD_TESTS=OFF \
    -D YAML_CPP_BUILD_CONTRIB=OFF \
    -D CMAKE_INSTALL_PREFIX=/usr/local \
    .. \
    && make ${PARALLEL_MAKE_ARG} && make install \
    && cd ../.. && rm -rf ./0.8.0.tar.gz ./yaml-cpp-0.8.0; \
    else \
    apt-get install -y libjemalloc2 libjemalloc-dev libyaml-cpp-dev; \
    fi

# Install Python
# - We use python-is-python3 because on Ubuntu 20.04 /usr/bin/python was removed
#   to aid in tracking down anything that depends on python 2. However, many
#   scripts use `/usr/bin/env python` to find python so restore it.
RUN apt-get update -y \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    apt-get install -y zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev \
    libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev \
    make \
    && wget https://www.python.org/ftp/python/3.10.1/Python-3.10.1.tgz \
    && tar -xf Python-3.10.1.tgz && cd ./Python-3.10.1 \
    && ./configure --enable-optimizations && make ${PARALLEL_MAKE_ARG} \
    && make install && cd ../ \
    && rm -rf ./Python-3.10.1.tgz ./Python-3.10.1; \
    else \
    apt-get install -y python3-pip python-is-python3 pkg-config; \
    fi

# Install Python packages
# We only install packages that are needed by the build system (e.g. to compile
# Python bindings or build documentation) or used by Python code that is
# unit-tested. Any other packages can be installed on-demand.
# - Install h5py explicitly from binary so that cross compilation is quicker.
# - Constrain numpy version to make sure it is binary compatible with h5py.
COPY support/Python/requirements.txt requirements.txt
COPY support/Python/dev_requirements.txt dev_requirements.txt
ENV DEBIAN_FRONTEND noninteractive
RUN python3 -m pip install --upgrade pip \
    && pip3 --no-cache-dir install --only-binary=h5py \
      -r requirements.txt -r dev_requirements.txt "numpy<2.0" \
    && rm requirements.txt dev_requirements.txt

# Install Julia for the SXS package
# This is optional, as the SXS package will download Julia on first use.
# However, installing Julia explicitly gives more control over the installation.
# - Set a consistent path for precompiled Julia packages so they are found when
#   CI runs as a different user
ENV JULIA_DEPOT_PATH "/usr/local/julia"
RUN if [ ${UBUNTU_VERSION} = 22.04 ]; then \
    curl -fsSL https://install.julialang.org | sh -s -- \
        -y --add-to-path=false \
    ; fi
ENV PATH="$PATH:$JULIA_DEPOT_PATH/bin"
# Call the SXS package so it precompiles Julia packages on first use, see:
# https://moble.github.io/PostNewtonian.jl/dev/interface/python/
RUN if [ ${UBUNTU_VERSION} = 22.04 ]; then \
    python3 -c "from sxs import julia" \
    ; fi

# Enable bash-completion by installing it and then adding it to the .bashrc file
RUN apt-get update -y \
    && apt-get install -y bash-completion \
    && printf "if [ -f /etc/bash_completion ] && ! shopt -oq posix; then\n\
    . /etc/bash_completion\nfi\n\n" >> /root/.bashrc

# WARNING: We switch to an empty container briefly!!!
#
# Cross-compile dependencies on the host system that take a long time to build
# when emulating the target architecture. The `xbuild` stage is an image with
# the same architecture as the host. It compiles the packages for the target
# architecture. Then we can copy the compiled packages into the `base` image.
#
# Note: because we only cross-compile the latest Ubuntu version we support,
# we just delete the executables on Ubuntu 18.04 and disable doxygen and ccache.
FROM --platform=$BUILDPLATFORM tonistiigi/xx AS xx
FROM --platform=$BUILDPLATFORM ubuntu:22.04 AS xbuild
WORKDIR /root
COPY --from=xx / /
ARG TARGETPLATFORM
ARG UBUNTU_VERSION=22.04
ARG PARALLEL_MAKE_ARG=-j4

# Install build dependencies for the host system
RUN apt-get update -y \
    && apt-get install -y cmake clang lld flex bison wget git

# Install build dependencies for the target architecture
#
# We need libstd++ to work on the target arch that we are cross-compiling for
RUN xx-apt-get update -y \
    && xx-apt-get install -y --no-install-recommends \
    libc6-dev libstdc++-10-dev

# - Ccache
RUN wget https://github.com/ccache/ccache/releases/download/v4.8.2/ccache-4.8.2.tar.gz -O ccache.tar.gz \
    && tar -xzf ccache.tar.gz \
    && mv ccache-* ccache \
    && cd ccache \
    && mkdir build && cd build \
    && cmake $(xx-clang --print-cmake-defines) \
        -D HAVE_ASM_SSE2=OFF -D HAVE_ASM_SSE41=OFF \
        -D HAVE_ASM_AVX2=OFF -D HAVE_ASM_AVX512=OFF \
        -D CMAKE_BUILD_TYPE=Release .. \
    && make $PARALLEL_MAKE_ARG
# - Doxygen
RUN wget https://github.com/doxygen/doxygen/archive/Release_1_9_3.tar.gz -O doxygen.tar.gz \
    && tar -xzf doxygen.tar.gz \
    && mv doxygen-* doxygen \
    && cd doxygen \
    && mkdir build && cd build \
    && cmake $(xx-clang --print-cmake-defines) \
        -D CMAKE_BUILD_TYPE=Release .. \
    && make $PARALLEL_MAKE_ARG

# WARNING: We now copy the things from the "xbuild" container/image into the
#          xbuild-test container to make sure the copying works. We do the
#          actual copy into the spectre container below.
#
FROM base AS xbuild-test
ARG UBUNTU_VERSION=22.04
COPY --from=xbuild /root/ccache/build/ccache /usr/local/bin
RUN ccache --version
COPY --from=xbuild /root/doxygen/build/bin/doxygen /usr/local/bin
RUN doxygen --version
COPY --from=xbuild /usr/local/bin/cmake /work/
RUN /work/cmake --version \
    && if [ ${UBUNTU_VERSION} = 18.04 ]; then \
    rm /usr/local/bin/ccache /usr/local/bin/doxygen; \
    else \
    mv /work/cmake /usr/local/bin; \
    fi


# WARNING: We are now back in the spectre container. We create two base
#          images for the 2 possible target architectures and then select
#          the one we need as the `dev` container below. This is essentially an
#          `if` in the Dockerfile.
#
# Install software that we can't install through apt. We have to distinguish
# between different architectures for many of those.
FROM base AS base-amd64
ENV CHARM_ARCH=x86_64
ENV TEX_ARCH=x86_64

FROM base AS base-arm64
ENV CHARM_ARCH=arm8
ENV TEX_ARCH=aarch64

FROM base-${TARGETARCH} AS dev
ARG TARGETARCH
ARG UBUNTU_VERSION=22.04

ARG PARALLEL_MAKE_ARG=-j4
ARG DEBIAN_FRONTEND=noninteractive

# We install dependencies not available through apt manually rather than using
# Spack since Spack ends up building a lot of dependencies from scratch
# that we don't need. Thus, not building the deps with Spack reduces total
# build time of the Docker image.
# - Blaze
RUN wget https://bitbucket.org/blaze-lib/blaze/downloads/blaze-3.8.tar.gz -O blaze.tar.gz \
    && tar -xzf blaze.tar.gz \
    && mv blaze-* blaze \
    && mv blaze/blaze /usr/local/include \
    && rm -rf blaze*
# - Catch2
RUN wget https://github.com/catchorg/Catch2/archive/refs/tags/v3.4.0.tar.gz -O catch.tar.gz \
    && tar -xzf catch.tar.gz && rm catch.tar.gz \
    && mv Catch2-* Catch2 \
    && cd Catch2 \
    && cmake -B build -D BUILD_TESTING=OFF \
        -D CMAKE_POSITION_INDEPENDENT_CODE=ON \
    && cd build \
    && make $PARALLEL_MAKE_ARG install \
    && cd ../.. && rm -rf Catch2
# - Ccache
COPY --from=xbuild /root/ccache/build/ccache /usr/local/bin
# - Doxygen
COPY --from=xbuild /root/doxygen/build/bin/doxygen /usr/local/bin
# - Libbacktrace
RUN git clone https://github.com/ianlancetaylor/libbacktrace \
    && cd libbacktrace \
    && ./configure --prefix=/usr/local \
    && make $PARALLEL_MAKE_ARG install \
    && cd .. && rm -rf libbacktrace
# - LibXSMM
RUN if [ "$TARGETARCH" = "arm64" ] ; then \
        git clone --single-branch --branch main --depth 1 \
            https://github.com/libxsmm/libxsmm.git libxsmm \
        && cd libxsmm \
        && make $PARALLEL_MAKE_ARG PREFIX=/usr/local/ PLATFORM=1 install \
        && cd .. \
        && rm -rf libxsmm; \
    else \
        wget https://github.com/hfp/libxsmm/archive/1.16.1.tar.gz \
            -O libxsmm.tar.gz \
        && tar -xzf libxsmm.tar.gz \
        && mv libxsmm-* libxsmm \
        && cd libxsmm \
        && make $PARALLEL_MAKE_ARG PREFIX=/usr/local/ install \
        && cd .. \
        && rm -rf libxsmm libxsmm.tar.gz; \
    fi
# - xsimd https://github.com/xtensor-stack/xsimd
RUN wget http://github.com/xtensor-stack/xsimd/archive/refs/tags/11.1.0.tar.gz -O xsimd.tar.gz \
    && tar -xzf xsimd.tar.gz && rm xsimd.tar.gz \
    && cd ./xsimd-*  \
    && mkdir build \
    && cd ./build \
    && cmake -D CMAKE_BUILD_TYPE=Release \
        -D CMAKE_INSTALL_PREFIX=/usr/local ../ \
    && make install \
    && cd ../.. && rm -rf xsimd-*

# Update ld cache to find shared libs in /usr/local/lib/
RUN ldconfig

WORKDIR /work
# Download and build the Charm++ version used by SpECTRE
# We check out only a specific branch in order to reduce the repo size.
#
# We remove the `doc` and `example` directories since these aren't useful to us
# in the container and we want to reduce the size of the container. We do NOT
# remove the `tmp` directories inside the Charm++ build directories because
# Charm++ stores non-temporary files (such as headers) that are needed when
# building with Charm++ in the `tmp` directories.
#
# We build  with debug symbols to make debugging Charm++-interoperability
# easier for people, and build with O2 to reduce build size.
RUN wget https://raw.githubusercontent.com/sxs-collaboration/spectre/develop/support/Charm/v7.0.0.patch

RUN git clone --single-branch --branch v7.0.1 --depth 1 \
        https://github.com/UIUC-PPL/charm charm_7_0_0 \
    && cd /work/charm_7_0_0 \
    && git checkout v7.0.1 \
    && git apply /work/v7.0.0.patch \
    && ./build charm++ multicore-linux-${CHARM_ARCH} gcc \
        ${PARALLEL_MAKE_ARG} -g -O2 --build-shared --with-production \
    && rm -r /work/charm_7_0_0/doc /work/charm_7_0_0/examples
ENV CHARM_ROOT="/work/charm_7_0_0/multicore-linux-${CHARM_ARCH}-gcc"

# Set the environment variable SPECTRE_CONTAINER so we can check if we are
# inside a container (0 is true in bash)
ENV SPECTRE_CONTAINER 0

# The singularity containers work better if the locale is set properly
RUN apt-get update -y \
    && apt-get install -y locales language-pack-fi language-pack-en \
    && export LANGUAGE=en_US.UTF-8 \
    && export LANG=en_US.UTF-8 \
    && export LC_ALL=en_US.UTF-8 \
    && locale-gen en_US.UTF-8 \
    && dpkg-reconfigure locales

# Install bibtex for Doxygen bibliography management
# We first install the TeXLive infrastructure according to the configuration in
# support/TeXLive/texlive.profile and then use it to install the bibtex package.
RUN if [ ${UBUNTU_VERSION} = 22.04 ]; then \
    mkdir /work/texlive && cd /work/texlive \
    && wget http://mirror.ctan.org/systems/texlive/tlnet/install-tl-unx.tar.gz \
    && tar -xzf install-tl-unx.tar.gz \
    && rm install-tl-unx.tar.gz \
    && wget https://raw.githubusercontent.com/sxs-collaboration/spectre/develop/support/TeXLive/texlive.profile \
    && install-tl-*/install-tl -profile=texlive.profile \
    && rm -r install-tl-* texlive.profile install-tl.log \
    && /work/texlive/bin/${TEX_ARCH}-linux/tlmgr install bibtex \
    ; fi
ENV PATH="${PATH}:/work/texlive/bin/${TEX_ARCH}-linux"

# Remove the apt-get cache in order to reduce image size
RUN apt-get -y clean

WORKDIR /work


# Deploy compiled executables to an image that can be run on HPC systems.
# - We could also compile in the dev container and then copy the executables to
#   a minimal deploy container to reduce its size. That requires keeping track
#   of the shared libraries that are needed to run the executables, so to make
#   things easier we just base the deploy container on the dev container.
# - We inherit from the remote image rather than the local dev because it is
#   faster on release CI to just pull the remote dev image rather than having to
#   build the dev container again. We could pre-fetch the dev image and then
#   build from the local cache, but that didn't work right away.
FROM sxscollaboration/spectre:dev AS deploy
ARG BUILDARCH
ARG TARGETARCH
ARG UBUNTU_VERSION=22.04
ARG PARALLEL_MAKE_ARG=-j2

COPY . spectre/

RUN mkdir spectre/build && cd spectre/build \
    && cmake \
    -D CMAKE_C_COMPILER=clang \
    -D CMAKE_CXX_COMPILER=clang++ \
    -D CMAKE_Fortran_COMPILER=gfortran \
    -D CMAKE_BUILD_TYPE=Release \
    -D DEBUG_SYMBOLS=OFF \
    -D BUILD_PYTHON_BINDINGS=ON \
    -D MEMORY_ALLOCATOR=SYSTEM \
    ..

# Skip compiling the executables if we are emulating the target architecture
# because that takes a long time. They can be compiled on-demand.
RUN if [ "$TARGETARCH" = "$BUILDARCH" ] ; then \
        cd spectre/build \
        && make ${PARALLEL_MAKE_ARG} cli \
        && make ${PARALLEL_MAKE_ARG} CharacteristicExtract \
        && make ${PARALLEL_MAKE_ARG} SolveXcts \
    ; fi

ENV SPECTRE_HOME /work/spectre
ENV PATH $SPECTRE_HOME/build/bin:$PATH
ENV PYTHONPATH $SPECTRE_HOME/build/bin/python:$PYTHONPATH

# Set the CLI as entrypoint
ENTRYPOINT ["spectre"]
CMD ["--help"]

# Build a demo image with extra software used in the tutorials.
FROM deploy AS demo
ARG BUILDARCH
ARG TARGETARCH
ARG UBUNTU_VERSION=22.04
ARG PARALLEL_MAKE_ARG=-j4

# vim and emacs for editing files
# Also ffmpeg for making movies with paraview output pngs
RUN apt-get update -y \
    && apt-get install -y vim emacs-nox ffmpeg

# Install headless paraview so we can run pvserver in the container
# Note: there is no arm64 linux binary of paraview available, so don't
# install paraview when building for Apple Silicon. Apple Silicon users
# should install a binary of ParaView for Mac and move data to be
# visualized outside of the container.
RUN if [ "$TARGETARCH" != "arm64" ] ; then \
        wget -O paraview.tar.gz --no-check-certificate "https://www.paraview.org/paraview-downloads/download.php?submit=Download&version=v5.10&type=binary&os=Linux&downloadFile=ParaView-5.10.1-osmesa-MPI-Linux-Python3.9-x86_64.tar.gz" \
        && tar -xzf paraview.tar.gz \
        && rm paraview.tar.gz \
        && mv ParaView-* /opt/paraview; \
    fi

ENV PATH "/opt/paraview/bin:$PATH"

# Build the executables used in the tutorial
RUN if [ "$TARGETARCH" = "$BUILDARCH" ] ; then \
        cd spectre/build \
        && make ${PARALLEL_MAKE_ARG} ExportCoordinates3D \
        && make ${PARALLEL_MAKE_ARG} EvolveScalarAdvection2D \
    ; fi

RUN pip3 --no-cache-dir install jupyterlab

# Used for compiling the static executables and libraries during CI so that we
# can release statically linked versions of our executables and libraries
# for other groups to use.
FROM sxscollaboration/spectre:dev1804 AS deploy_static_execs_and_libs
ARG BUILDARCH
ARG TARGETARCH
ARG UBUNTU_VERSION=18.04
ARG PARALLEL_MAKE_ARG=-j4

WORKDIR /work

COPY . spectre/

# Build SpECTRE with as many static libraries as possible. Because we cannot
# statically link to glibc (it is unsupported and causes a lot of undefined
# behavior) we build using Ubuntu 18.04 to support a fairly old version of
# glibc.
RUN if [ ${UBUNTU_VERSION} != 18.04 ] && [ "$TARGETARCH" != "amd64" ]; then \
    echo "Must use Ubuntu 18.04 with AMD64 for deploying static execs" \
    && exit 1; fi \
    && mkdir spectre/build && cd spectre/build \
    && cmake \
    -D CMAKE_C_COMPILER=gcc-9 \
    -D CMAKE_CXX_COMPILER=g++-9 \
    -D CMAKE_Fortran_COMPILER=gfortran-9 \
    -D CMAKE_BUILD_TYPE=Release \
    -D DEBUG_SYMBOLS=OFF \
    -D BUILD_PYTHON_BINDINGS=OFF \
    -D MEMORY_ALLOCATOR=SYSTEM \
    -D BUILD_SHARED_LIBS=OFF \
    -D ENABLE_PARAVIEW=OFF \
    -D Boost_USE_STATIC_LIBS=ON \
    -D BLA_STATIC=ON \
    -D GSL_STATIC=ON \
    -D HDF5_USE_STATIC_LIBRARIES=ON \
    -D SPECTRE_Fortran_STATIC_LIBS=ON \
    -D YAMLCPP_STATIC_LIBRARY=ON \
    -D USE_GIT_HOOKS=OFF \
    -D OVERRIDE_ARCH=haswell \
    -D USE_LD=gold \
    -D gfortran=/usr/lib/gcc/x86_64-linux-gnu/9/libgfortran.a \
    -D quadmath=/usr/lib/gcc/x86_64-linux-gnu/9/libquadmath.a \
    -D BUILD_DOCS=OFF \
    -D USE_CCACHE=OFF \
    .. \
    && make ${PARALLEL_MAKE_ARG} CharacteristicExtract ReduceCceWorldtube \
    && ctest -LE unit -R CharacteristicExtract
